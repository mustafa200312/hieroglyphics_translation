{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f2e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19016c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0764d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Cleaned_Data/transcription_to_hieroglyphs.csv\")\n",
    "dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cba7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['transcription', 'hieroglyphs'],\n",
       "    num_rows: 35252\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df130b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model = \"google/gemma-3-270m-it\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, device_map=\"auto\", attn_implementation=\"eager\", dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "print(f\"Device: {base_model.device}\")\n",
    "print(f\"DType: {base_model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c95482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934759bb0243498a8c9b639e76f2eacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def translate(sample):\n",
    "  return {\n",
    "      \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": \"Translate this transliteration to hieroglyphics: \"},\n",
    "          {\"role\": \"user\", \"content\": f\"{sample['transcription']}\"},\n",
    "          {\"role\": \"assistant\", \"content\": f\"{sample['hieroglyphs']}\"}\n",
    "      ]\n",
    "  }\n",
    "\n",
    "training_dataset = dataset.map(translate, remove_columns=dataset.features.keys())\n",
    "training_dataset_splits = training_dataset.train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d7b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Translate this transliteration to hieroglyphics: ', 'role': 'system'}, {'content': 'jy.n =j m nʾ,t =j hꜣi̯.n =j m spꜣ,t =j jri̯.n =j ḥzz.t nṯr =j mrr.t nṯr.pl =j nb.w', 'role': 'user'}, {'content': 'M18 N35 A1 G17 O49 A1 G1 O4 D54 N35 A1 G17 N24C X1 Z1 A1 D4 N35 M17 A1 V28 W14 O34 O34 X1 Y2 A40 M17 A1 U6 D21 D21 X1 R8 A40 Z2 M17 A1 N35 V30 G43', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223c45e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 31726\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c295b11",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4e90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3457ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"../adapters/transliteration-to-hero-adapters\"      # Where to save your LoRA adapters\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",                      # Target all linear layers\n",
    "    lora_dropout=0.05,                                # Increase to 0.1 to induce overfitting\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"]       # Save the lm_head and embed_tokens as you train the special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "452e258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=adapter_path,                          # Directory to save adapters\n",
    "    num_train_epochs=3,                               # Number of training epochs\n",
    "    per_device_train_batch_size=4,                    # Batch size per device during training\n",
    "    logging_strategy=\"epoch\",                         # Log every epoch\n",
    "    eval_strategy=\"epoch\",                            # Evaluate loss metrics every epoch\n",
    "    save_strategy=\"epoch\",                            # Save checkpoint every epoch\n",
    "    learning_rate=5e-5,                               # Learning rate,\n",
    "    lr_scheduler_type=\"constant\",                     # Use constant learning rate scheduler\n",
    "    max_length=256,                                   # Max sequence length for model and packing of the dataset\n",
    "    gradient_checkpointing=False,                     # Use gradient checkpointing to save memory\n",
    "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
    "    optim=\"adamw_torch_fused\",                        # Use fused adamw optimizer\n",
    "    report_to=\"tensorboard\",                          # Report metrics to tensorboard\n",
    "    weight_decay=0.01,                                # Added weight decay for regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fa3c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9220bcf87d4c28985059ef808baf29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(gemma_model, quantization_config=bnb_config, device_map=\"auto\", attn_implementation='eager')\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba47569",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = training_dataset_splits['test']\n",
    "train_dataset = training_dataset_splits['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8a479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "eval_dataset = train_dataset['test']\n",
    "train_dataset = train_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d37694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\hieroglyphics_project\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:1222: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc38b8012f1a4e119fe36ac89881a9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/28553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337da3e66ad5449392c69fd59c2d5983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/28553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f95aa37506455c92fbce52e4244e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/3173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90bb56a6b5c4948b3f75a4cab8ece0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/3173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b4ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16065' max='21417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16065/21417 7:33:16 < 2:31:01, 0.59 it/s, Epoch 2.25/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.465123</td>\n",
       "      <td>1.308555</td>\n",
       "      <td>1.321567</td>\n",
       "      <td>3031140.000000</td>\n",
       "      <td>0.642588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.237214</td>\n",
       "      <td>1.208536</td>\n",
       "      <td>1.160613</td>\n",
       "      <td>6062280.000000</td>\n",
       "      <td>0.675316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(adapter_path)\n",
    "print(f\"LoRA adapters saved to {adapter_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
