{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5909f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a4cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Checkpoint dir: ../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"../Cleaned_data/transcription_to_hieroglyphs.csv\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "\n",
    "D_MODEL = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.1\n",
    "MAX_POSITIONS = 4096\n",
    "\n",
    "CHECKPOINT_DIR = \"../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Checkpoint dir:\", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8beed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 31726\n",
      "Val size: 3526\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>hieroglyphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27276</th>\n",
       "      <td>ḏi̯.t = n =k ꜥš 2 _ḫr 12</td>\n",
       "      <td>D37 X1 N35 V31 D36 N37 Z7 W22 Z7 Z1 Z1 Aa1 D21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10081</th>\n",
       "      <td>m bẖs,w ḥr ḥḏ,w</td>\n",
       "      <td>G17 D58 F32 O34 Z7 X4 D2 Z1 T3 I10 \"⸮\" G43 \"\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33867</th>\n",
       "      <td>ꜥnḫ jt =j Rꜥw-Ḥr,w-ꜣḫ,tj-ḥꜥi̯-m-ꜣḫ,t M-rn≡f-m-...</td>\n",
       "      <td>S34 M17 X1 I9 A40 A40 &lt; S34 G9 N27 N27 V28 D36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11326</th>\n",
       "      <td>ꜥḥꜥ.n Pꜣ-Rꜥ-Ḥr,w-ꜣḫ,tj.du ḥr ḏd n Ḏḥw,tj</td>\n",
       "      <td>P6 D36 N35 G41 G1 N5 G7 G5 G7 N27 X1 Z4 O1 O1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>snd,w ḥr mꜣj ḥnꜥ ꜥꜣm.w.pl rḫ =f sw r =f jw =f ...</td>\n",
       "      <td>S29 N35 D46 Z7 G54 A2 Z3A D2 Z1 U1 G1 M17 Z7 F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           transcription  \\\n",
       "27276                           ḏi̯.t = n =k ꜥš 2 _ḫr 12   \n",
       "10081                                    m bẖs,w ḥr ḥḏ,w   \n",
       "33867  ꜥnḫ jt =j Rꜥw-Ḥr,w-ꜣḫ,tj-ḥꜥi̯-m-ꜣḫ,t M-rn≡f-m-...   \n",
       "11326           ꜥḥꜥ.n Pꜣ-Rꜥ-Ḥr,w-ꜣḫ,tj.du ḥr ḏd n Ḏḥw,tj   \n",
       "3451   snd,w ḥr mꜣj ḥnꜥ ꜥꜣm.w.pl rḫ =f sw r =f jw =f ...   \n",
       "\n",
       "                                             hieroglyphs  \n",
       "27276  D37 X1 N35 V31 D36 N37 Z7 W22 Z7 Z1 Z1 Aa1 D21...  \n",
       "10081  G17 D58 F32 O34 Z7 X4 D2 Z1 T3 I10 \"⸮\" G43 \"\" ...  \n",
       "33867  S34 M17 X1 I9 A40 A40 < S34 G9 N27 N27 V28 D36...  \n",
       "11326  P6 D36 N35 G41 G1 N5 G7 G5 G7 N27 X1 Z4 O1 O1 ...  \n",
       "3451   S29 N35 D46 Z7 G54 A2 Z3A D2 Z1 U1 G1 M17 Z7 F...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df = df.dropna(subset=[\"transcription\", \"hieroglyphs\"])\n",
    "df[\"transcription\"] = df[\"transcription\"].astype(str)\n",
    "df[\"hieroglyphs\"] = df[\"hieroglyphs\"].astype(str)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eef7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5000\n",
      "Special IDs: {'PAD': 0, 'SOS': 1, 'EOS': 2, 'UNK': 3}\n",
      "Saved tokenizer to: ../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\\tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=5000,\n",
    "    special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    train_df[\"transcription\"].tolist() + train_df[\"hieroglyphs\"].tolist(),\n",
    "    trainer\n",
    ")\n",
    "\n",
    "PAD = tokenizer.token_to_id(\"<pad>\")\n",
    "SOS = tokenizer.token_to_id(\"<sos>\")\n",
    "EOS = tokenizer.token_to_id(\"<eos>\")\n",
    "UNK = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n",
    "print(\"Special IDs:\", {\"PAD\": PAD, \"SOS\": SOS, \"EOS\": EOS, \"UNK\": UNK})\n",
    "\n",
    "# Save tokenizer once\n",
    "tokenizer_path = os.path.join(CHECKPOINT_DIR, \"tokenizer.json\")\n",
    "tokenizer.save(tokenizer_path)\n",
    "print(\"Saved tokenizer to:\", tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5f8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, sos_id, eos_id):\n",
    "        self.src = df[\"transcription\"].tolist()\n",
    "        self.tgt = df[\"hieroglyphs\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sos = sos_id\n",
    "        self.eos = eos_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = [self.sos] + self.tokenizer.encode(self.src[idx]).ids + [self.eos]\n",
    "        tgt_ids = [self.sos] + self.tokenizer.encode(self.tgt[idx]).ids + [self.eos]\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    src, tgt = zip(*batch)\n",
    "    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=pad_id)\n",
    "    tgt = nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=pad_id)\n",
    "    return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f881b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Seq2SeqDataset(train_df, tokenizer, SOS, EOS)\n",
    "val_ds   = Seq2SeqDataset(val_df, tokenizer, SOS, EOS)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.src_emb = nn.Embedding(vocab_size, D_MODEL, padding_idx=pad_id)\n",
    "        self.tgt_emb = nn.Embedding(vocab_size, D_MODEL, padding_idx=pad_id)\n",
    "\n",
    "        self.pos_enc = PositionalEncoding(D_MODEL, DROPOUT, MAX_POSITIONS)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=NHEAD,\n",
    "            num_encoder_layers=NUM_LAYERS,\n",
    "            num_decoder_layers=NUM_LAYERS,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(D_MODEL, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_pad_mask = (src == self.pad_id)\n",
    "        tgt_pad_mask = (tgt == self.pad_id)\n",
    "\n",
    "        src = self.pos_enc(self.src_emb(src) * math.sqrt(D_MODEL))\n",
    "        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(D_MODEL))\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src, tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            memory_key_padding_mask=src_pad_mask\n",
    "        )\n",
    "\n",
    "        return self.fc_out(out)  # (B,T,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34deefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_decode_ids(batch_ids, tokenizer, skip_special_tokens=True):\n",
    "    # batch_ids: (B, T) int64 numpy or list of lists\n",
    "    texts = []\n",
    "    for ids in batch_ids:\n",
    "        # tokenizers.Tokenizer supports decode(ids, skip_special_tokens=...)\n",
    "        texts.append(tokenizer.decode(list(map(int, ids)), skip_special_tokens=skip_special_tokens))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def token_f1_order_free(pred_text, gold_text):\n",
    "    # Same logic as your compute_metrics: whitespace split + Counter overlap\n",
    "    pred_tokens = pred_text.strip().split()\n",
    "    gold_tokens = gold_text.strip().split()\n",
    "\n",
    "    p_cnt = Counter(pred_tokens)\n",
    "    g_cnt = Counter(gold_tokens)\n",
    "\n",
    "    tp = sum((p_cnt & g_cnt).values())\n",
    "    precision = tp / max(1, len(pred_tokens))\n",
    "    recall = tp / max(1, len(gold_tokens))\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694c0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        tgt_y  = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), tgt_y.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss_acc_and_tokenf1(model, loader, criterion, pad_id):\n",
    "    \"\"\"\n",
    "    - val_loss: teacher forcing loss\n",
    "    - avg_token_acc: micro token accuracy over ALL non-pad tokens in dataset\n",
    "    - token_f1: average of per-row order-free token F1 on decoded text (no CER)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    total_f1 = 0.0\n",
    "    n_rows = 0\n",
    "\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        tgt_y  = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), tgt_y.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = logits.argmax(dim=-1)  # (B,T)\n",
    "\n",
    "        # --- avg token accuracy (micro over dataset) ---\n",
    "        mask = (tgt_y != pad_id)\n",
    "        total_correct += (preds[mask] == tgt_y[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        # --- token_f1 per row (decoded) ---\n",
    "        # Prepare numpy arrays for decoding\n",
    "        preds_np = preds.detach().cpu().numpy()\n",
    "        labels_np = tgt_y.detach().cpu().numpy()\n",
    "\n",
    "        # replace pad id in labels where pad (already pad_id), decoding with skip_special_tokens removes them\n",
    "        pred_texts = batch_decode_ids(preds_np, tokenizer, skip_special_tokens=True)\n",
    "        gold_texts = batch_decode_ids(labels_np, tokenizer, skip_special_tokens=True)\n",
    "\n",
    "        for ptxt, gtxt in zip(pred_texts, gold_texts):\n",
    "            total_f1 += token_f1_order_free(ptxt, gtxt)\n",
    "            n_rows += 1\n",
    "\n",
    "    val_loss = total_loss / max(1, len(loader))\n",
    "    avg_token_acc = (total_correct / total_tokens) if total_tokens > 0 else 0.0\n",
    "    token_f1 = (total_f1 / n_rows) if n_rows > 0 else 0.0\n",
    "\n",
    "    return val_loss, avg_token_acc, token_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acb62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n"
     ]
    }
   ],
   "source": [
    "model = TransformerSeq2Seq(VOCAB_SIZE, PAD).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd6a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, model, optimizer, epoch, train_loss, val_loss, avg_token_acc, token_f1):\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"avg_token_acc\": avg_token_acc,\n",
    "        \"token_f1\": token_f1,\n",
    "        \"special_ids\": {\"PAD\": PAD, \"SOS\": SOS, \"EOS\": EOS, \"UNK\": UNK},\n",
    "        \"config\": {\n",
    "            \"D_MODEL\": D_MODEL,\n",
    "            \"NHEAD\": NHEAD,\n",
    "            \"NUM_LAYERS\": NUM_LAYERS,\n",
    "            \"DROPOUT\": DROPOUT,\n",
    "            \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "        }\n",
    "    }\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, map_location=DEVICE):\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    if optimizer is not None and \"optimizer_state\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\hieroglyphics_project\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\projects\\hieroglyphics_project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train_loss=3.9502 | val_loss=3.3469 | avg_token_acc=0.3211 | token_f1=0.3137 | epoch_time=2m 20s | elapsed=2m 20s | ETA=9m 20s | saved=../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\\epoch_001.pt\n",
      "Epoch 2/5 | train_loss=3.1929 | val_loss=2.9559 | avg_token_acc=0.3811 | token_f1=0.3990 | epoch_time=1m 55s | elapsed=4m 16s | ETA=6m 23s | saved=../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\\epoch_002.pt\n",
      "Epoch 3/5 | train_loss=2.8939 | val_loss=2.7323 | avg_token_acc=0.4140 | token_f1=0.5026 | epoch_time=2m 4s | elapsed=6m 21s | ETA=4m 13s | saved=../best_weights/transcription_to_hieroglyphs_transformer_checkpoints\\epoch_003.pt\n"
     ]
    }
   ],
   "source": [
    "def fmt(sec):\n",
    "    sec = int(sec)\n",
    "    h = sec // 3600\n",
    "    m = (sec % 3600) // 60\n",
    "    s = sec % 60\n",
    "    if h > 0:\n",
    "        return f\"{h}h {m}m {s}s\"\n",
    "    if m > 0:\n",
    "        return f\"{m}m {s}s\"\n",
    "    return f\"{s}s\"\n",
    "\n",
    "\n",
    "start_all = time.time()\n",
    "epoch_times = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, avg_token_acc, token_f1 = eval_loss_acc_and_tokenf1(model, val_loader, criterion, PAD)\n",
    "\n",
    "    epoch_sec = time.time() - start_epoch\n",
    "    epoch_times.append(epoch_sec)\n",
    "\n",
    "    elapsed_sec = time.time() - start_all\n",
    "    avg_epoch_sec = sum(epoch_times) / len(epoch_times)\n",
    "    eta_sec = (EPOCHS - epoch) * avg_epoch_sec\n",
    "\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, f\"epoch_{epoch:03d}.pt\")\n",
    "    save_checkpoint(ckpt_path, model, optimizer, epoch, train_loss, val_loss, avg_token_acc, token_f1)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(best_path, model, optimizer, epoch, train_loss, val_loss, avg_token_acc, token_f1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "        f\"avg_token_acc={avg_token_acc:.4f} | token_f1={token_f1:.4f} | \"\n",
    "        f\"epoch_time={fmt(epoch_sec)} | elapsed={fmt(elapsed_sec)} | ETA={fmt(eta_sec)} | \"\n",
    "        f\"saved={ckpt_path}\"\n",
    "    )\n",
    "\n",
    "print(\"Best checkpoint:\", best_path, \"best_val_loss:\", best_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
