{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe62532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saifk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\saifk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734537f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"bbaw_egyptian_w_hieroglyphs_translated.csv\" \n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "\n",
    "OUTPUT_DIR = \"./bart_gardiner_en\"\n",
    "MAX_INPUT_LEN = 128\n",
    "MAX_TARGET_LEN = 64\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492bafba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>translation</th>\n",
       "      <th>hieroglyphs</th>\n",
       "      <th>translation_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jr,j-pÍú•,t ·∏•Íú£,tj-Íú• ·∏´tm,w-bj,tj smr-wÍú•,tj jm,j-r...</td>\n",
       "      <td>Hereditary noble and prince, royal seal-bearer...</td>\n",
       "      <td>D21 Q3 D36 F4 D36 L2 X1 S19 S29 U23 T21 X1 G17...</td>\n",
       "      <td>Hereditary noble and prince, royal seal-bearer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j Íú•n·∏´.w.pl tp,j.pl-tÍú£ swÍú£.t =sn ·∏•r jz pn m-·∏´di...</td>\n",
       "      <td>O living ones, who are upon the earth, who sha...</td>\n",
       "      <td>M17 A26 S34 Aa1 G43 A1 Z3 h N17 N23 A1 Z2B S29...</td>\n",
       "      <td>O living ones, who are upon the earth, who sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jnk m·∏•-jb-n-nswt m ·∏•w,t-n·πØr r æ-N·∏´n m pr Stj,t ...</td>\n",
       "      <td>I was a trusted one of the king in the temple,...</td>\n",
       "      <td>W24 V31 V22 F34 N35 M23 X1 N35 G17 R8 O6 X1 O1...</td>\n",
       "      <td>I was a trusted one of the king in the temple,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Íú•q jb.pl ·∏•r-sÍú£ mr,yt jm,j-r æ-Íú•·∏•Íú•,w-wr-m-pr-nsw...</td>\n",
       "      <td>A trusted one upon the landing place, great ov...</td>\n",
       "      <td>G35 F34 F34 F34 D2 Z1 Aa17 U6 D21 M17 M17 X1 N...</td>\n",
       "      <td>A trusted one upon the landing place, great ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jw jriÃØ.n = jz m ·∏•z,t nswt sjqr.n wj ·∏•m =f m t...</td>\n",
       "      <td>I built a tomb through the favour of the king,...</td>\n",
       "      <td>M17 G43 D4 N35 M17 M40 O34 O1 Z1 G17 V28 W14 X...</td>\n",
       "      <td>I built a tomb through the favour of the king,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35498</th>\n",
       "      <td>wr‚∏Æ.pl n p·∏•,tj =k</td>\n",
       "      <td>Die Gro√üen ... werden fallen ... aufgrund dein...</td>\n",
       "      <td>G36 D21 A21 N35 F4 V31</td>\n",
       "      <td>The great ones will fall because of your strength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35499</th>\n",
       "      <td>hmhm =k mj hh nsr.t m-sÍú£ ·∏´Íú£s,t nb.t</td>\n",
       "      <td>Dein Kriegsschrei folgt wie der Gluthauch des ...</td>\n",
       "      <td>O4 G17 O4 G17 A2 Z2 V31 W19 M17 O4 O4 Q7 N35 F...</td>\n",
       "      <td>Your war cry follows like the glowing breath o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35500</th>\n",
       "      <td>·∏´Íú£s,t nb.t twt.w m jb wÍú•.tj fkw =sn tÍú£ =sn m-mn,t</td>\n",
       "      <td>... jedes Fremdland ist eintr√§chtig eines einz...</td>\n",
       "      <td>N25 X1 Z1 V30 X1 X1 G43 X1 G43 Y1 Z2 Aa15 F34 ...</td>\n",
       "      <td>Every foreign land is united in a single wish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35501</th>\n",
       "      <td>r zj.tw ·πØÍú£w r fn·∏è =sn jn kÍú£ =k</td>\n",
       "      <td>... damit durch deinen Ka Atem f√ºr ihre Nasen ...</td>\n",
       "      <td>D21 O35 Z4A D54 X1 G43 P5 G43 Z2 D21 D20 O34 N...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35502</th>\n",
       "      <td>nb-TÍú£.du WÍú•-n-RÍú•w jriÃØ.y kÍú£ =k r p·∏• tw</td>\n",
       "      <td>O Herr der Beiden L√§nder, Wa-en-Ra! M√∂ge dein ...</td>\n",
       "      <td>N21 N21 &lt; N5 T21 N35 &gt; D4 M17 M17 G43 D28 Z1 V...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35503 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           transcription  \\\n",
       "0      jr,j-pÍú•,t ·∏•Íú£,tj-Íú• ·∏´tm,w-bj,tj smr-wÍú•,tj jm,j-r...   \n",
       "1      j Íú•n·∏´.w.pl tp,j.pl-tÍú£ swÍú£.t =sn ·∏•r jz pn m-·∏´di...   \n",
       "2      jnk m·∏•-jb-n-nswt m ·∏•w,t-n·πØr r æ-N·∏´n m pr Stj,t ...   \n",
       "3      Íú•q jb.pl ·∏•r-sÍú£ mr,yt jm,j-r æ-Íú•·∏•Íú•,w-wr-m-pr-nsw...   \n",
       "4      jw jriÃØ.n = jz m ·∏•z,t nswt sjqr.n wj ·∏•m =f m t...   \n",
       "...                                                  ...   \n",
       "35498                                  wr‚∏Æ.pl n p·∏•,tj =k   \n",
       "35499                hmhm =k mj hh nsr.t m-sÍú£ ·∏´Íú£s,t nb.t   \n",
       "35500  ·∏´Íú£s,t nb.t twt.w m jb wÍú•.tj fkw =sn tÍú£ =sn m-mn,t   \n",
       "35501                     r zj.tw ·πØÍú£w r fn·∏è =sn jn kÍú£ =k   \n",
       "35502             nb-TÍú£.du WÍú•-n-RÍú•w jriÃØ.y kÍú£ =k r p·∏• tw   \n",
       "\n",
       "                                             translation  \\\n",
       "0      Hereditary noble and prince, royal seal-bearer...   \n",
       "1      O living ones, who are upon the earth, who sha...   \n",
       "2      I was a trusted one of the king in the temple,...   \n",
       "3      A trusted one upon the landing place, great ov...   \n",
       "4      I built a tomb through the favour of the king,...   \n",
       "...                                                  ...   \n",
       "35498  Die Gro√üen ... werden fallen ... aufgrund dein...   \n",
       "35499  Dein Kriegsschrei folgt wie der Gluthauch des ...   \n",
       "35500  ... jedes Fremdland ist eintr√§chtig eines einz...   \n",
       "35501  ... damit durch deinen Ka Atem f√ºr ihre Nasen ...   \n",
       "35502  O Herr der Beiden L√§nder, Wa-en-Ra! M√∂ge dein ...   \n",
       "\n",
       "                                             hieroglyphs  \\\n",
       "0      D21 Q3 D36 F4 D36 L2 X1 S19 S29 U23 T21 X1 G17...   \n",
       "1      M17 A26 S34 Aa1 G43 A1 Z3 h N17 N23 A1 Z2B S29...   \n",
       "2      W24 V31 V22 F34 N35 M23 X1 N35 G17 R8 O6 X1 O1...   \n",
       "3      G35 F34 F34 F34 D2 Z1 Aa17 U6 D21 M17 M17 X1 N...   \n",
       "4      M17 G43 D4 N35 M17 M40 O34 O1 Z1 G17 V28 W14 X...   \n",
       "...                                                  ...   \n",
       "35498                             G36 D21 A21 N35 F4 V31   \n",
       "35499  O4 G17 O4 G17 A2 Z2 V31 W19 M17 O4 O4 Q7 N35 F...   \n",
       "35500  N25 X1 Z1 V30 X1 X1 G43 X1 G43 Y1 Z2 Aa15 F34 ...   \n",
       "35501  D21 O35 Z4A D54 X1 G43 P5 G43 Z2 D21 D20 O34 N...   \n",
       "35502  N21 N21 < N5 T21 N35 > D4 M17 M17 G43 D28 Z1 V...   \n",
       "\n",
       "                                          translation_en  \n",
       "0      Hereditary noble and prince, royal seal-bearer...  \n",
       "1      O living ones, who are upon the earth, who sha...  \n",
       "2      I was a trusted one of the king in the temple,...  \n",
       "3      A trusted one upon the landing place, great ov...  \n",
       "4      I built a tomb through the favour of the king,...  \n",
       "...                                                  ...  \n",
       "35498  The great ones will fall because of your strength  \n",
       "35499  Your war cry follows like the glowing breath o...  \n",
       "35500  Every foreign land is united in a single wish,...  \n",
       "35501                                                NaN  \n",
       "35502                                                NaN  \n",
       "\n",
       "[35503 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ebb64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 35496\n"
     ]
    }
   ],
   "source": [
    "assert \"hieroglyphs\" in df.columns\n",
    "assert \"translation_en\" in df.columns\n",
    "\n",
    "# Basic cleaning\n",
    "df[\"hieroglyphs\"] = (\n",
    "    df[\"hieroglyphs\"]\n",
    "    .astype(str)\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "df[\"translation_en\"] = (\n",
    "    df[\"translation_en\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[df[\"hieroglyphs\"] != \"\"]\n",
    "df = df[df[\"translation_en\"] != \"\"]\n",
    "\n",
    "print(f\"Dataset size after cleaning: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fb4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068c8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartScaledWordEmbedding(52452, 768, padding_idx=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optional: Add Gardiner codes to tokenizer vocab\n",
    "unique_codes = set(\n",
    "    \" \".join(df[\"hieroglyphs\"].tolist()).split()\n",
    ")\n",
    "\n",
    "tokenizer.add_tokens(list(unique_codes))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d12521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/28396 [00:00<?, ? examples/s]c:\\Users\\saifk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28396/28396 [00:24<00:00, 1136.07 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3550/3550 [00:03<00:00, 1171.19 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3550/3550 [00:02<00:00, 1231.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    inputs = [\n",
    "        f\"Translate hieroglyphs to English: {x}\"\n",
    "        for x in batch[\"hieroglyphs\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"translation_en\"],\n",
    "            max_length=MAX_TARGET_LEN,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # üî¥ THIS LINE FIXES EVERYTHING\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "    labels_ids = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in seq]\n",
    "        for seq in labels_ids\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104327b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4991db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2500,\n",
    "    save_steps=2500,\n",
    "\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "\n",
    "    eval_accumulation_steps=4,   # üî• FIX MEMORY GROWTH\n",
    "\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea658199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saifk\\AppData\\Local\\Temp\\ipykernel_24664\\341744725.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17750' max='17750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17750/17750 1:01:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.087800</td>\n",
       "      <td>2.896055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.860500</td>\n",
       "      <td>2.681912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.591400</td>\n",
       "      <td>2.545922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.457100</td>\n",
       "      <td>2.433786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.319600</td>\n",
       "      <td>2.355024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.238700</td>\n",
       "      <td>2.296510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.245900</td>\n",
       "      <td>2.272027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saifk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17750, training_loss=2.6305291447169346, metrics={'train_runtime': 3714.1882, 'train_samples_per_second': 38.226, 'train_steps_per_second': 4.779, 'total_flos': 1.08212977926144e+16, 'train_loss': 2.6305291447169346, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b74d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3550 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3550/3550 [14:12<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08728294071672292\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "for sample in tqdm(test_df.itertuples(), total=len(test_df)):\n",
    "    text = f\"Translate hieroglyphs to English: {sample.hieroglyphs}\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=40,\n",
    "            num_beams=1\n",
    "        )\n",
    "\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    preds.append(pred)\n",
    "    refs.append([sample.translation_en])\n",
    "\n",
    "result = bleu.compute(predictions=preds, references=refs)\n",
    "print(\"BLEU:\", result[\"bleu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63bb5662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Accuracy: 0.0683\n"
     ]
    }
   ],
   "source": [
    "def token_accuracy(predictions, references):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "\n",
    "        min_len = min(len(pred_tokens), len(ref_tokens))\n",
    "\n",
    "        for i in range(min_len):\n",
    "            if pred_tokens[i] == ref_tokens[i]:\n",
    "                correct += 1\n",
    "\n",
    "        total += len(ref_tokens)\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "token_acc = token_accuracy(preds, [r[0] for r in refs])\n",
    "print(f\"Token Accuracy: {token_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ecd4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='888' max='888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [888/888 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 2.298457622528076, 'eval_runtime': 24.9593, 'eval_samples_per_second': 142.232, 'eval_steps_per_second': 35.578, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "print(\"Test metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0609d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b29077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: The ruler of the two countries ,  Nefer-cheperu-Re-wa-en-re\n"
     ]
    }
   ],
   "source": [
    "def translate_gardiner(sequence: str):\n",
    "    text = f\"Translate hieroglyphs to English: {sequence.upper()}\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Example usage\n",
    "example_sequence = \"G36 D21 A21 N35 F4 V31\"\n",
    "translation = translate_gardiner(example_sequence)\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a81442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8496,\n",
       "  'G36 D21 A21 N35 F4 V31',\n",
       "  'The great ones will fall because of your strength')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = [\n",
    "    (i, h, en)\n",
    "    for i, (h, en) in enumerate(\n",
    "        zip(dataset[\"train\"][\"hieroglyphs\"],\n",
    "            dataset[\"train\"][\"translation_en\"])\n",
    "    )\n",
    "    if h == \"G36 D21 A21 N35 F4 V31\"\n",
    "]\n",
    "\n",
    "matches[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9825b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
